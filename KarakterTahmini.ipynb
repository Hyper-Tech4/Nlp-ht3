{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVELDTrgqcJ3",
        "outputId": "73621703-4e32-410a-caf6-6166ef18bba6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQYH_dxsTYAA",
        "outputId": "5088f5dc-a6f1-4ead-c096-c2f7aa8b43fb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 2s/step - accuracy: 0.5138 - loss: 0.6924 - val_accuracy: 0.5977 - val_loss: 0.6709\n",
            "Epoch 2/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 2s/step - accuracy: 0.6573 - loss: 0.6153 - val_accuracy: 0.7019 - val_loss: 0.5539\n",
            "Epoch 3/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 2s/step - accuracy: 0.7645 - loss: 0.4761 - val_accuracy: 0.7630 - val_loss: 0.4881\n",
            "Epoch 4/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m809s\u001b[0m 2s/step - accuracy: 0.8198 - loss: 0.3771 - val_accuracy: 0.7795 - val_loss: 0.4479\n",
            "Epoch 5/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 2s/step - accuracy: 0.8473 - loss: 0.3235 - val_accuracy: 0.7842 - val_loss: 0.4453\n",
            "Epoch 6/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 2s/step - accuracy: 0.8552 - loss: 0.2950 - val_accuracy: 0.7917 - val_loss: 0.4507\n",
            "Epoch 7/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 2s/step - accuracy: 0.8686 - loss: 0.2667 - val_accuracy: 0.7949 - val_loss: 0.4341\n",
            "Epoch 8/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m855s\u001b[0m 2s/step - accuracy: 0.8717 - loss: 0.2550 - val_accuracy: 0.7934 - val_loss: 0.4653\n",
            "Epoch 9/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 2s/step - accuracy: 0.8762 - loss: 0.2439 - val_accuracy: 0.8002 - val_loss: 0.4810\n",
            "Epoch 10/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m828s\u001b[0m 2s/step - accuracy: 0.8786 - loss: 0.2329 - val_accuracy: 0.7929 - val_loss: 0.4765\n",
            "Epoch 11/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m835s\u001b[0m 2s/step - accuracy: 0.8843 - loss: 0.2240 - val_accuracy: 0.8038 - val_loss: 0.5290\n",
            "Epoch 12/30\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 2s/step - accuracy: 0.8823 - loss: 0.2215 - val_accuracy: 0.8049 - val_loss: 0.5145\n",
            "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 423ms/step - accuracy: 0.7854 - loss: 0.5282\n",
            "Test Loss: 0.5186375975608826\n",
            "Test Accuracy: 78.15%\n",
            "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 416ms/step\n",
            "Accuracy: 0.7815\n",
            "Precision: 0.7843\n",
            "Recall: 0.7815\n",
            "F1 Score: 0.7800\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import chardet\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        rawdata = f.read()\n",
        "    result = chardet.detect(rawdata)\n",
        "    return result['encoding']\n",
        "\n",
        "file_path = '/content/drive/MyDrive/friends.csv'\n",
        "\n",
        "encoding = detect_encoding(file_path)\n",
        "df = pd.read_csv(file_path, encoding=encoding)\n",
        "\n",
        "df = df[df['character'].isin(['Chandler', 'Rachel'])]\n",
        "\n",
        "df['dialogue'] = df['dialogue'].fillna('')\n",
        "df['dialogue'] = df['dialogue'].astype(str)\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df['dialogue'] = df['dialogue'].apply(preprocess_text)\n",
        "\n",
        "df_combined = pd.concat([df, df], ignore_index=True)\n",
        "\n",
        "df_train, df_test = train_test_split(df_combined, test_size=0.2, random_state=42, stratify=df_combined['character'])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['character_encoded'] = label_encoder.fit_transform(df_train['character'])\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(df_train['dialogue'])\n",
        "sequences_train = tokenizer.texts_to_sequences(df_train['dialogue'])\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test['dialogue'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "max_seq_length = 120\n",
        "X_train = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_test = pad_sequences(sequences_test, maxlen=max_seq_length)\n",
        "y_train = df_train['character_encoded'].values\n",
        "\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=200, input_length=max_seq_length))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.4)))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "\n",
        "model.load_weights('best_model.keras')\n",
        "\n",
        "df_test['character_encoded'] = label_encoder.transform(df_test['character'])\n",
        "y_test = df_test['character_encoded'].values\n",
        "\n",
        "if len(X_test) != len(y_test):\n",
        "    X_test = X_test[:len(y_test)]\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VGfvu6TJeqn-"
      }
    }
  ]
}