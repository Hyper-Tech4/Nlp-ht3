{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9131793,"sourceType":"datasetVersion","datasetId":5513662}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install charset-normalizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T19:57:37.484029Z","iopub.execute_input":"2024-08-08T19:57:37.484388Z","iopub.status.idle":"2024-08-08T19:57:54.468015Z","shell.execute_reply.started":"2024-08-08T19:57:37.484359Z","shell.execute_reply":"2024-08-08T19:57:54.466238Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: charset-normalizer in /opt/conda/lib/python3.10/site-packages (3.3.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install chardet","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:58:22.079119Z","iopub.execute_input":"2024-08-08T19:58:22.079553Z","iopub.status.idle":"2024-08-08T19:58:37.503106Z","shell.execute_reply.started":"2024-08-08T19:58:22.079515Z","shell.execute_reply":"2024-08-08T19:58:37.501746Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: chardet\nSuccessfully installed chardet-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport chardet\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\ndef detect_encoding(file_path):\n    with open(file_path, 'rb') as f:\n        rawdata = f.read()\n    result = chardet.detect(rawdata)\n    return result['encoding']\n\n\nfile_path = '/kaggle/input/friends/friends.csv'\nencoding = detect_encoding(file_path)\ndf = pd.read_csv(file_path, encoding=encoding)\n\n\ndf = df[df['character'].isin(['Chandler', 'Rachel'])]\n\n\ndf['dialogue'] = df['dialogue'].fillna('')  \ndf['dialogue'] = df['dialogue'].astype(str)  \n\n\ndef preprocess_text(text):\n    text = text.lower()  \n    text = re.sub(r'[^\\w\\s]', '', text)  \n\ndf['dialogue'] = df['dialogue'].apply(preprocess_text)\n\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['dialogue'])\nword_index = tokenizer.word_index\nmax_seq_length = 120  \n\n\nsequences = tokenizer.texts_to_sequences(df['dialogue'])\nX = pad_sequences(sequences, maxlen=max_seq_length)\n\n\nlabel_encoder = LabelEncoder()\ndf['character_encoded'] = label_encoder.fit_transform(df['character'])\ny = df['character_encoded'].values\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=max_seq_length))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(150, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(150, dropout=0.3, recurrent_dropout=0.3)))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(len(label_encoder.classes_), activation='softmax'))\n\n\noptimizer = Adam(learning_rate=0.0005)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nhistory = model.fit(X, y, epochs=15, batch_size=32, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n\n\ngc.collect()\n\n\nmodel = load_model('best_model.keras')\n\ndef predict_character(sentence):\n    processed_sentence = preprocess_text(sentence)\n    sequence = tokenizer.texts_to_sequences([processed_sentence])\n    padded_sequence = pad_sequences(sequence, maxlen=max_seq_length)\n    prediction = model.predict(padded_sequence, verbose=0)\n    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n    return predicted_label[0], prediction[0]\n\n\ndef build_text_generation_model(vocab_size, seq_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=seq_length))\n    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n    model.add(Dropout(0.3))\n    model.add(Bidirectional(LSTM(150)))\n    model.add(Dense(150, activation='relu'))\n    model.add(Dense(vocab_size, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef generate_sentence_from_model(model, tokenizer, seed_text, seq_length, num_words):\n    result = seed_text\n    for _ in range(num_words):\n        token_list = tokenizer.texts_to_sequences([result])[0]\n        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n        predicted_probs = model.predict(token_list, verbose=0)\n        predicted = np.argmax(predicted_probs, axis=-1)\n        output_word = tokenizer.index_word.get(predicted[0], '')\n        if output_word == '':\n            break\n        result += \" \" + output_word\n    return result\n\n\ndef generate_sentences_for_character(character):\n    character_dialogues = df[df['character'] == character]['dialogue'].tolist()\n    text_tokenizer = Tokenizer()\n    text_tokenizer.fit_on_texts(character_dialogues)\n    total_words = len(text_tokenizer.word_index) + 1\n\n    \n    input_sequences = []\n    for line in character_dialogues:\n        token_list = text_tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n\n    max_seq_length_text = max([len(x) for x in input_sequences])\n    input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length_text, padding='pre')\n    X_text, y_text = input_sequences[:,:-1], input_sequences[:,-1]\n\n  \n    y_text = np.eye(total_words)[y_text]\n\n\n    text_model = build_text_generation_model(total_words, max_seq_length_text-1)\n    text_model.fit(X_text, y_text, epochs=20, verbose=1)\n\n\n    seed_text = \"Merhaba\"  \n    generated_sentence = generate_sentence_from_model(text_model, text_tokenizer, seed_text, max_seq_length_text-1, 10)\n    return generated_sentence\n\n\nuser_input = input(\"Bir cümle girin: \")\npredicted_character, prediction_probs = predict_character(user_input)\nprint(f\"Tahmin edilen karakter: {predicted_character}\")\n\n\ngenerated_sentence = generate_sentences_for_character(predicted_character)\nprint(f\"{predicted_character} için üretilen cümle: {generated_sentence}\")\n\n\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T20:26:33.394541Z","iopub.execute_input":"2024-08-08T20:26:33.395028Z","iopub.status.idle":"2024-08-09T00:04:46.881185Z","shell.execute_reply.started":"2024-08-08T20:26:33.394996Z","shell.execute_reply":"2024-08-09T00:04:46.879852Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 553ms/step - accuracy: 0.5402 - loss: 0.6908 - val_accuracy: 0.5812 - val_loss: 0.6701\nEpoch 2/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 550ms/step - accuracy: 0.6638 - loss: 0.6201 - val_accuracy: 0.5919 - val_loss: 0.6686\nEpoch 3/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 540ms/step - accuracy: 0.7710 - loss: 0.4770 - val_accuracy: 0.5721 - val_loss: 0.7506\nEpoch 4/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 543ms/step - accuracy: 0.8299 - loss: 0.3703 - val_accuracy: 0.5831 - val_loss: 0.8405\nEpoch 5/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 544ms/step - accuracy: 0.8469 - loss: 0.3166 - val_accuracy: 0.5699 - val_loss: 0.9402\nEpoch 6/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 547ms/step - accuracy: 0.8546 - loss: 0.2881 - val_accuracy: 0.5793 - val_loss: 0.9649\nEpoch 7/15\n\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 554ms/step - accuracy: 0.8716 - loss: 0.2566 - val_accuracy: 0.5844 - val_loss: 1.2538\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Bir cümle girin:  Şimdi bu küçük oyunda anne mi oluyorum?\n"},{"name":"stdout","text":"Tahmin edilen karakter: Chandler\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 368ms/step - accuracy: 0.0324 - loss: 8.0053\nEpoch 2/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 375ms/step - accuracy: 0.0406 - loss: 7.4040\nEpoch 3/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 369ms/step - accuracy: 0.0509 - loss: 7.2048\nEpoch 4/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 370ms/step - accuracy: 0.0540 - loss: 7.0299\nEpoch 5/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 369ms/step - accuracy: 0.0651 - loss: 6.8356\nEpoch 6/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 368ms/step - accuracy: 0.0711 - loss: 6.6567\nEpoch 7/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 368ms/step - accuracy: 0.0828 - loss: 6.4836\nEpoch 8/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 369ms/step - accuracy: 0.0916 - loss: 6.3141\nEpoch 9/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 378ms/step - accuracy: 0.0948 - loss: 6.1808\nEpoch 10/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 377ms/step - accuracy: 0.1037 - loss: 6.0095\nEpoch 11/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 377ms/step - accuracy: 0.1107 - loss: 5.8739\nEpoch 12/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 370ms/step - accuracy: 0.1157 - loss: 5.7292\nEpoch 13/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 371ms/step - accuracy: 0.1229 - loss: 5.5632\nEpoch 14/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 371ms/step - accuracy: 0.1291 - loss: 5.4280\nEpoch 15/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 386ms/step - accuracy: 0.1341 - loss: 5.3045\nEpoch 16/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 372ms/step - accuracy: 0.1428 - loss: 5.1392\nEpoch 17/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 367ms/step - accuracy: 0.1459 - loss: 5.0252\nEpoch 18/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 362ms/step - accuracy: 0.1538 - loss: 4.8968\nEpoch 19/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 362ms/step - accuracy: 0.1622 - loss: 4.7713\nEpoch 20/20\n\u001b[1m1524/1524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 367ms/step - accuracy: 0.1654 - loss: 4.6558\nChandler için üretilen cümle: Merhaba bu gece bir şey var mı yoksa bir şey var\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"81569"},"metadata":{}}]}]}
