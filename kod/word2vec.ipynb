{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9131857,"sourceType":"datasetVersion","datasetId":5513709}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Load the dataset\nfile_path = '/kaggle/input/friends/friends.csv'  # Update with your file path\ndf = pd.read_csv(file_path, encoding='utf-8')\n\n# Filter for Chandler and Rachel's dialogues\ndf = df[df['character'].isin(['Chandler', 'Rachel'])]\n\n# Fill missing dialogues with empty strings and convert to lowercase\ndf['dialogue'] = df['dialogue'].fillna('').astype(str).apply(lambda x: x.lower())\n\n# Remove punctuation\ndf['dialogue'] = df['dialogue'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n# Save preprocessed text to a file\ndf['dialogue'].to_csv('friends_dialogues.txt', index=False, header=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T18:32:37.245986Z","iopub.execute_input":"2024-08-08T18:32:37.247296Z","iopub.status.idle":"2024-08-08T18:32:37.530150Z","shell.execute_reply.started":"2024-08-08T18:32:37.247250Z","shell.execute_reply":"2024-08-08T18:32:37.528689Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import logging\nimport multiprocessing\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n\n# Train Word2Vec model\ninput_file = 'friends_dialogues.txt'\noutput_file = 'friends_word2vec_model'\n\n# Use vector_size instead of size\nmodel = Word2Vec(LineSentence(input_file), vector_size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())\nmodel.wv.save_word2vec_format(output_file, binary=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T18:33:42.201419Z","iopub.execute_input":"2024-08-08T18:33:42.201940Z","iopub.status.idle":"2024-08-08T18:33:43.966567Z","shell.execute_reply.started":"2024-08-08T18:33:42.201900Z","shell.execute_reply":"2024-08-08T18:33:43.965228Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom imblearn.over_sampling import RandomOverSampler\nfrom gensim.models import KeyedVectors\n\n# Load and preprocess data\nfile_path = '/kaggle/input/friends/friends.csv'\ndf = pd.read_csv(file_path, encoding='utf-8')\ndf = df[df['character'].isin(['Chandler', 'Rachel'])]\ndf['dialogue'] = df['dialogue'].fillna('').astype(str)\ndf['dialogue'] = df['dialogue'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n\ndf_combined = pd.concat([df, df], ignore_index=True)\ndf_train, df_test = train_test_split(df_combined, test_size=0.2, random_state=42, stratify=df_combined['character'])\n\nlabel_encoder = LabelEncoder()\ndf_train['character_encoded'] = label_encoder.fit_transform(df_train['character'])\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['dialogue'])\nsequences_train = tokenizer.texts_to_sequences(df_train['dialogue'])\nsequences_test = tokenizer.texts_to_sequences(df_test['dialogue'])\nword_index = tokenizer.word_index\n\n# Load Word2Vec embeddings\nword2vec_path = 'friends_word2vec_model'\nword2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\n# Create embedding matrix\nembedding_dim = 400\nvocab_size = len(word_index) + 1\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\n# Prepare data for training\nmax_seq_length = 120\nX_train = pad_sequences(sequences_train, maxlen=max_seq_length)\nX_test = pad_sequences(sequences_test, maxlen=max_seq_length)\ny_train = df_train['character_encoded'].values\n\nros = RandomOverSampler(random_state=42)\nX_train, y_train = ros.fit_resample(X_train, y_train)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.4)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(len(label_encoder.classes_), activation='softmax'))\n\noptimizer = Adam(learning_rate=0.0003)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n\nmodel.load_weights('best_model.keras')\n\ndf_test['character_encoded'] = label_encoder.transform(df_test['character'])\ny_test = df_test['character_encoded'].values\n\nif len(X_test) != len(y_test):\n    X_test = X_test[:len(y_test)]\n\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\nprecision = precision_score(y_test, y_pred_classes, average='weighted')\nrecall = recall_score(y_test, y_pred_classes, average='weighted')\nf1 = f1_score(y_test, y_pred_classes, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T18:34:01.167320Z","iopub.execute_input":"2024-08-08T18:34:01.168636Z","iopub.status.idle":"2024-08-08T20:40:33.104089Z","shell.execute_reply.started":"2024-08-08T18:34:01.168579Z","shell.execute_reply":"2024-08-08T20:40:33.102344Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-08-08 18:34:03.719086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-08 18:34:03.719288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-08 18:34:03.907109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2s/step - accuracy: 0.5276 - loss: 0.6920 - val_accuracy: 0.4218 - val_loss: 0.7094\nEpoch 2/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 2s/step - accuracy: 0.5225 - loss: 0.6889 - val_accuracy: 0.5254 - val_loss: 0.6909\nEpoch 3/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 2s/step - accuracy: 0.5247 - loss: 0.6864 - val_accuracy: 0.5389 - val_loss: 0.6928\nEpoch 4/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 2s/step - accuracy: 0.5356 - loss: 0.6852 - val_accuracy: 0.5705 - val_loss: 0.6905\nEpoch 5/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 2s/step - accuracy: 0.5318 - loss: 0.6856 - val_accuracy: 0.5611 - val_loss: 0.6914\nEpoch 6/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 2s/step - accuracy: 0.5375 - loss: 0.6829 - val_accuracy: 0.5808 - val_loss: 0.6865\nEpoch 7/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - accuracy: 0.5438 - loss: 0.6830 - val_accuracy: 0.5624 - val_loss: 0.6917\nEpoch 8/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - accuracy: 0.5427 - loss: 0.6826 - val_accuracy: 0.5639 - val_loss: 0.6865\nEpoch 9/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.5442 - loss: 0.6819 - val_accuracy: 0.6015 - val_loss: 0.6749\nEpoch 10/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 2s/step - accuracy: 0.5457 - loss: 0.6822 - val_accuracy: 0.5692 - val_loss: 0.6851\nEpoch 11/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 2s/step - accuracy: 0.5533 - loss: 0.6804 - val_accuracy: 0.5141 - val_loss: 0.6969\nEpoch 12/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - accuracy: 0.5479 - loss: 0.6806 - val_accuracy: 0.5256 - val_loss: 0.6906\nEpoch 13/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - accuracy: 0.5622 - loss: 0.6788 - val_accuracy: 0.5265 - val_loss: 0.6913\nEpoch 14/30\n\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 2s/step - accuracy: 0.5554 - loss: 0.6793 - val_accuracy: 0.5711 - val_loss: 0.6859\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 342ms/step - accuracy: 0.5394 - loss: 0.6843\nTest Loss: 0.6834892630577087\nTest Accuracy: 54.32%\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 346ms/step\nAccuracy: 0.5432\nPrecision: 0.5832\nRecall: 0.5432\nF1 Score: 0.5078\n","output_type":"stream"}]}]}
